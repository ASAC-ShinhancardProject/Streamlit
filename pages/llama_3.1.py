import streamlit as st
import requests
import json
import subprocess
import io
from pptx import Presentation
from PyPDF2 import PdfReader

# í•¨ìˆ˜ ì •ì˜
def read_ppt(file):
    prs = Presentation(file)
    text_content = []
    for slide in prs.slides:
        for shape in slide.shapes:
            if hasattr(shape, 'text'):
                text_content.append(shape.text)
    return '\n'.join(text_content)

def read_pdf(file):
    pdf = PdfReader(file)
    text_content = []
    for page in pdf.pages:
        text_content.append(page.extract_text())
    return '\n'.join(text_content)

def check_ollama_status():
    try:
        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
        if result.returncode == 0:
            return "Ollamaê°€ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤."
        else:
            return f"Ollama ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {result.stderr}"
    except FileNotFoundError:
        return "Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•Šê±°ë‚˜ PATHì— ì—†ìŠµë‹ˆë‹¤."

def generate_llama_response(prompt_input):
    if "ê·¸ë ¤" in prompt_input or "ê·¸ë¦¼" in prompt_input:
        return {
            "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì €ëŠ” í…ìŠ¤íŠ¸ ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ë¡œ, ì‹¤ì œë¡œ ê·¸ë¦¼ì„ ê·¸ë¦¬ê±°ë‚˜ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì‹  ê·¸ë¦¼ì— ëŒ€í•œ ì„¤ëª…ì„ ì œê³µí•˜ê±°ë‚˜ ê´€ë ¨ ì •ë³´ë¥¼ ì•Œë ¤ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ ë„ì›€ì´ í•„ìš”í•˜ì‹ ê°€ìš”?",
            "is_error": False
        }
    
    string_dialogue = f"You are a helpful assistant. Your responses should be in a {response_style.lower()} style. Respond in Korean. You are a text-based AI assistant and cannot generate, produce, edit, manipulate or create images.\n\n"
    for message in st.session_state.messages:
        if isinstance(message, dict):
            string_dialogue += f"{message['role'].capitalize()}: {message['content']}\n\n"
        elif hasattr(message, "type") and hasattr(message, "content"):
            string_dialogue += f"{message.type.capitalize()}: {message.content}\n\n"
    string_dialogue += f"Human: {prompt_input}\n\nAssistant: "
    
    data = {
        "model": selected_model,
        "prompt": string_dialogue,
        "stream": True,
        "temperature": temperature,
        "max_tokens": max_length,
    }
    
    try:
        response = requests.post(f"{OLLAMA_URL}/api/generate", json=data, stream=True, timeout=30)
        response.raise_for_status()
        return {"response": response, "is_error": False}
    except requests.exceptions.Timeout:
        return {"response": "API ìš”ì²­ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. Ollama ì„œë²„ì˜ ìƒíƒœë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.", "is_error": True}
    except requests.exceptions.ConnectionError:
        return {"response": "Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.", "is_error": True}
    except requests.exceptions.RequestException as e:
        return {"response": f"API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", "is_error": True}

def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"}]
    if 'uploaded_file_content' in st.session_state:
        del st.session_state.uploaded_file_content
    if 'uploaded_file_name' in st.session_state:
        del st.session_state.uploaded_file_name

# App title
st.set_page_config(page_title="ğŸ¦™ğŸ’¬ Llama Chatbot")

# Sidebar contents
with st.sidebar:
    st.title('ğŸ¦™ğŸ’¬ Llama Chatbot')
    st.write('This chatbot uses the Llama model via Ollama.')
    
    st.subheader('Models and parameters')
    selected_model = st.selectbox('Choose a model', 
                                  ['llama3.1:latest', 'llama3:latest'],
                                  key='selected_model')
    temperature = st.slider('Temperature', min_value=0.1, max_value=2.0, value=0.1, step=0.1)
    max_length = st.slider('Max Length', min_value=64, max_value=4096, value=1024, step=64)
    response_style = st.selectbox('Response Style', ['Formal', 'Casual', 'Professional'], index=0)

# Ollama ì„œë²„ URL ì„¤ì •
OLLAMA_URL = st.sidebar.text_input("Ollama ì„œë²„ URL", "http://localhost:11434")

# Store LLM generated responses
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"}]

# Display chat messages
for message in st.session_state.messages:
    if isinstance(message, dict):
        role = message.get("role", "assistant")
        content = message.get("content", "")
    elif hasattr(message, "type") and hasattr(message, "content"):
        role = message.type
        content = message.content
    else:
        st.warning(f"Unexpected message format: {message}")
        continue
    
    with st.chat_message(role):
        st.write(content)

st.sidebar.button('Clear Chat History', on_click=clear_chat_history, key='clear_chat_history')

# User-provided prompt
prompt = st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”")

# íŒŒì¼ ì—…ë¡œë“œ ë° ë¶„ì„ ê¸°ëŠ¥
with st.expander("íŒŒì¼ ì—…ë¡œë“œ ë° ë¶„ì„", expanded=False):
    uploaded_file = st.file_uploader("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["txt", "pdf", "ppt", "pptx"])
    if uploaded_file is not None:
        file_contents = uploaded_file.read()
        
        if uploaded_file.type == "application/pdf":
            file_text = read_pdf(io.BytesIO(file_contents))
        elif uploaded_file.type in ["application/vnd.ms-powerpoint", "application/vnd.openxmlformats-officedocument.presentationml.presentation"]:
            file_text = read_ppt(io.BytesIO(file_contents))
        else:
            # ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„
            encodings = ['utf-8', 'euc-kr', 'cp949', 'iso-8859-1']
            file_text = None
            for encoding in encodings:
                try:
                    file_text = file_contents.decode(encoding)
                    break
                except UnicodeDecodeError:
                    continue
            
            # ëª¨ë“  ì¸ì½”ë”© ì‹œë„ ì‹¤íŒ¨ ì‹œ ë°”ì´ë„ˆë¦¬ ëª¨ë“œë¡œ ì½ê¸°
            if file_text is None:
                file_text = str(file_contents)
        
        st.session_state.uploaded_file_content = file_text
        st.session_state.uploaded_file_name = uploaded_file.name
        st.success(f"íŒŒì¼ '{uploaded_file.name}'ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # íŒŒì¼ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
        with st.expander("íŒŒì¼ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°", expanded=False):
            st.text_area("File Content", value=file_text, height=300, max_chars=None, key="file_content_preview")
        
    file_analysis = st.button("íŒŒì¼ ë¶„ì„")

if prompt or (file_analysis and 'uploaded_file_content' in st.session_state):
    if file_analysis and 'uploaded_file_content' in st.session_state:
        file_prompt = f"ë‹¤ìŒ íŒŒì¼ '{st.session_state.uploaded_file_name}'ì˜ ë‚´ìš©ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:\n\n{st.session_state.uploaded_file_content}\n\nì‚¬ìš©ì ì§ˆë¬¸: {prompt if prompt else 'íŒŒì¼ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.'}"
        st.session_state.messages.append({"role": "user", "content": f"íŒŒì¼ '{st.session_state.uploaded_file_name}' ë¶„ì„ ìš”ì²­\nì‚¬ìš©ì ì§ˆë¬¸: {prompt if prompt else 'íŒŒì¼ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.'}"})
        with st.chat_message("user"):
            st.write(f"íŒŒì¼ '{st.session_state.uploaded_file_name}' ë¶„ì„ ìš”ì²­")
            if prompt:
                st.write(f"ì‚¬ìš©ì ì§ˆë¬¸: {prompt}")
        prompt = file_prompt
    elif prompt:
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)

    # Generate a new response
    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        full_response = ""
        response_placeholder.markdown("ë‹µë³€ ìƒì„± ì¤‘...")  # ë‹µë³€ ìƒì„± ì¤‘ í‘œì‹œ
        result = generate_llama_response(prompt)
        if result["is_error"]:
            st.error(result["response"])
        else:
            response = result["response"]
            if isinstance(response, str):  # ì§ì ‘ ë°˜í™˜ëœ ë¬¸ìì—´ ì‘ë‹µ ì²˜ë¦¬
                full_response = response
                response_placeholder.markdown(full_response)
                st.session_state.messages.append({"role": "assistant", "content": full_response})
            else:  # API ì‘ë‹µ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
                try:
                    for chunk in response.iter_lines():
                        if chunk:
                            chunk_data = json.loads(chunk.decode())
                            full_response += chunk_data['response']
                            response_placeholder.markdown(full_response)
                    
                    if full_response:
                        st.session_state.messages.append({"role": "assistant", "content": full_response})
                    else:
                        st.warning("ëª¨ë¸ì´ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
                except json.JSONDecodeError as e:
                    st.error(f"ì‘ë‹µ ë°ì´í„° íŒŒì‹± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                except KeyError as e:
                    st.error(f"ì‘ë‹µ ë°ì´í„°ì— í•„ìš”í•œ í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
                except Exception as e:
                    st.error(f"ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# Ollama ìƒíƒœ í™•ì¸ ë° í‘œì‹œ
ollama_status = check_ollama_status()
st.sidebar.write(f"Ollama ìƒíƒœ: {ollama_status}")